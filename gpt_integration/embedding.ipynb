{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-3 Text Embedding and Semantic Search\n",
    "\n",
    "This notebook demonstrates how to utilize the power of GPT-3 for text embedding and perform semantic search to find the most similar documents based on a user query.\n",
    "\n",
    "The GPT-3 language model is a state-of-the-art deep learning model developed by OpenAI. It has been trained on a vast amount of text data and can generate high-quality text responses.\n",
    "\n",
    "In this notebook, we will perform the following steps:\n",
    "\n",
    "1. Embedding Texts with GPT-3: We will use GPT-3 to embed a collection of texts into vector representations. These embeddings capture the semantic meaning of the texts.\n",
    "\n",
    "2. Chunking Texts: If the text documents are larger than the maximum token length supported by GPT-3, we will split them into smaller chunks for efficient processing.\n",
    "\n",
    "3. Semantic Search: Given a user query, we will calculate the similarity between the query embedding and the embeddings of the text chunks. We will identify and rank the most similar documents based on their similarity scores.\n",
    "\n",
    "4. Display Results: We will display the top-ranked documents that are most relevant to the user's query.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processor import Preprocessor, TextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHAT_COMPLETION_CTX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ronald\\TUM Materialien\\Semester 8\\seba nlp\\ki-suscheck\\gpt_integration\\embedding.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m embedder \u001b[39m=\u001b[39m TextEmbedder(txt_file\u001b[39m=\u001b[39mtxt_file, output_dir\u001b[39m=\u001b[39mchunked_txt_dir)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Setting the chunk size to be half of the max of the chat completion length. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# The reason is because the max length is equal to query + knowledge base + response. \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m chunk_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(CHAT_COMPLETION_CTX_LENGTH\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Uncomment the below lines when embedding the chunks is needed. \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# chunks = embedder.chunk_text(EMBEDDING_ENCODING, chunk_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# embed the chunks\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# embedder.embed_chunks(embeddings_file=embeddings_file)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ronald/TUM%20Materialien/Semester%208/seba%20nlp/ki-suscheck/gpt_integration/embedding.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(embeddings_file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CHAT_COMPLETION_CTX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "docx_file = 'data/FiBL-Bericht_Basiskonzept-KErn-final_en_preprocessed.docx'  \n",
    "# Location where the txt format of the report should be saved. \n",
    "txt_file = 'data/sustainability_report.txt'\n",
    "chunked_txt_dir = 'data/report_chunks'\n",
    "embeddings_file = 'data/embeddings.csv'\n",
    "\n",
    "#PREPROCESSING STEP: \n",
    "preprocessor = Preprocessor(docx_file=docx_file)\n",
    "# Uncomment the below line when preprocesssing is needed. \n",
    "# preprocessor.preprocess()\n",
    "\n",
    "# CHUNK STEP:\n",
    "# chunk the report\n",
    "embedder = TextEmbedder(txt_file=txt_file, output_dir=chunked_txt_dir)\n",
    "\n",
    "# Setting the chunk size to be half of the max of the chat completion length. \n",
    "# The reason is because the max length is equal to query + knowledge base + response. \n",
    "chunk_size = int(CHAT_COMPLETION_CTX_LENGTH/2)\n",
    "\n",
    "# Uncomment the below lines when embedding the chunks is needed. \n",
    "# chunks = embedder.chunk_text(EMBEDDING_ENCODING, chunk_size)\n",
    "# embed the chunks\n",
    "# embedder.embed_chunks(embeddings_file=embeddings_file)\n",
    "\n",
    "df = pd.read_csv(embeddings_file)\n",
    "\n",
    "query = 'How is the sustainability score calculated?'\n",
    "embedder.search_chunks(embeddings_file=embeddings_file, query=query, k=3, pprint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Other steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the query processing function. Similar to the gpt notebook. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seba-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
